# CSE151A-Project
### Project Overview:
This project will explore the potential for predicting a user’s rating of a recipe based on textual analysis of their comment as well as their potential history of comments and ratings on other recipes. We chose the problem of predicting how a given user will rate a recipe as the recommender system we are building would give insight into the qualities that are both desired or unwanted in specific recipes. Furthermore, a successful model would allow for improved recipe recommendations based off of a user’s past reviews. We will be using methods such as logistic regression to perform sentiment analysis on text to get the general sentiment of the text, as well as utilizing other features of the data such as the most important words or length of the text, we will fit these features on a regression model to determine if these features are an accurate portrayal of a user’s rating on a recipe. 
### Data Preprocessing:
- Missing Values
  - Drop the two missing text entries from the dataset to maintain data consistency.
- Text Tokenization and Cleaning
  - Remove special characters, numbers, and punctuation from the text attribute.
  - Convert all text to lowercase, tokenize the text into individual words, and stored the tokenized text into a list for futher analysis.
- Encoding
  - Encode the categorical attributes, recipe_name and user_name, using one hot encoding for the use in the model. 
- Scaling
  - Scale the numerical attributes, reply_count, thumbs up/down, best_score, and user_reputation, using normalization to ensure range.
- Splitting Data
  - Split the dataset into training and testing sets using an 80:20 ratio to evaluate model performance and avoid overfitting.
### Jupyter Notebook
https://colab.research.google.com/drive/1B-XxFEforvRwHhIIadXGSZB1HqIw5l3e?usp=sharing

# Milestone 3 Work
## Analysis of First Model

For our first model we utilized a SGDClassifier, which is a linear classifier that utilizes stochastic gradient descent. This model takes in the td-idf features of the processed text data and outputs a prediction of the stars (rating) category for the given review. In order to deal with the skew in our dataset towards five-star reviews, we utilized SMOTE to oversample our data and give a more uniform sample sapce to operate on. Our first model yielded a 83% accuracy on the training set and a 59% accuracy on the testing set. The combination of a high accuracy on the training set and a significant difference (24%) between the training and testing accuracy indicates that this model is most likely suffering from overfitting. As there are five different outcomes for this classifier (1-5 stars), having a testing accuracy of 59% is well above the random threshold of 20%. 

## Fitting Graph Location

Based off of our current error rates, we see a low error on the training set and a comparatively higher error on the test set. Based off of the fitting graph, this would indicate we are beyond the ideal range for model complexity and must seek to reduce the complexity of our model to help lessen the gap between the testing and training erorr. Our model is relatively high in complexity as a result of the sheer number of features which are generated by the if-idf values. In the future, we will seek to reduce this aspect of complexity by continuing to increase the granularity of the cleaning of text feature. Through the reduction of false duplicates ('hamburger' vs 'hamberger') and error text ('freerange' vs 'free range') we will reduce the number of tf-idf features and thus create a more generalizable model via the reduction of model complexity. Furthermore, we believe that including some other features such as overall sentiment or review length could also positively impact the accuracy of our model without significantly increasing the overall model complexity. 

## Future Models
In the future, we are considering looking at potentially utilizing a Naive Bayes model such as Sci-Kit Learn's MultinomalNB model. The Naive Bayes model seems ideal as it is able to efficiently handle the volume of data and hand and will effectively utilize the sparse nature of the tf-idf feature vectors more effectively than the linear regression classifier. Furthermore, we are also interested in looking into utilizing a Random Forest model to see if the leveraging of decision trees would help to aid in more effective classification in the context of having an extreme number of features. 

## Conclusion
Overall, our first model served as a good launch point for our investigation of this dataset. The SGDClassifier yielded a relatively low training error and a *comparatively* higher testing error! This indicates that there is significant room to grow in terms of changing our features and underlying model in order to venture closer to the ideal level of model complexity which would yield a more generalizable model. Based off of these error rates, we can conclude that utilization of the TF-IDF of words yields a better prediction than randomly selecting a rating; however, there is still significant room to improve and increase the model's overall efficacy. More specifically, we believe that the model could be improved by cleaning the text data even more thoroughly to help increase the accuracy of the TF-IDF values. This would help to reduce the complexity of our model by decreasing the number of features we are inputting. Furthermore, we are seeking to explore the inclusion of other attributes of the text as features in our model. Whether this surrounds something simple, such as review length, or something more complicated, such as sentiment, we hope that pulling more information from the review text column will help to improve our model! 

